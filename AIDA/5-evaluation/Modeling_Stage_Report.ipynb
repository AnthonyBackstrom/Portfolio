{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12ffb476-1c8b-4b11-8a4c-85ae9676fe7b",
   "metadata": {},
   "source": [
    "# Modeling Stage Report in the CRISP-DM Model\n",
    "\n",
    "* Author: Aleksandr Baranov\n",
    "* Date: 9.12.2023\n",
    "\n",
    "The Modeling stage is the fourth step in the CRISP-DM (Cross-Industry Standard Process for Data Mining) Model. This stage involves the development and evaluation of models to address the business problem. Here is a summary of the key activities:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f61d7a-d6f9-4059-b363-621d40508c19",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Selection of Modeling Methods:\n",
    "The choice of modeling methods depends on the nature of the problem and the type of data. Some common methods include:\n",
    "\n",
    "* Linear Regression: Suitable for predicting numeric values based on linear relationships between variables.\n",
    "\n",
    "* Logistic Regression: Applied for classification tasks, especially binary classification.\n",
    "\n",
    "* Decision Trees and Random Forests: Suitable for classification and regression tasks, capable of capturing non-linear relationships.\n",
    "\n",
    "* Support Vector Machines (SVM): Effective in separating data into classes.\n",
    "\n",
    "* Neural Networks: Useful for complex tasks requiring the learning of intricate patterns.\n",
    "\n",
    "* LightGBM is a gradient boosting algorithm that excels in regression and classification tasks. \n",
    "\n",
    "We also tested random forest, linear regression, catboost, xgboost and Lightgbm models, of which Lightgbm was found to be the best for our task.\n",
    "\n",
    "It possesses the following characteristics:\n",
    "\n",
    "#### Regression Tasks:\n",
    "\n",
    "LightGBM is well-suited for predicting numeric values, as it can capture complex nonlinear relationships between variables. This makes it a powerful tool for estimating prices, for example, based on the features of automobiles.\n",
    "Classification Tasks:\n",
    "\n",
    "LightGBM also performs exceptionally well in classification tasks, including binary and multiclass scenarios. Its ability to handle large datasets and identify intricate patterns makes it effective for tasks like classifying types of vehicles or determining product categories.\n",
    "\n",
    "#### Efficiency and Speed:\n",
    "\n",
    "LightGBM is known for its efficiency and speed. It employs gradient boosting techniques and is tree-based, allowing it to efficiently learn from large volumes of data and achieve high accuracy.\n",
    "\n",
    "#### Handling Categorical Features:\n",
    "\n",
    "The algorithm supports categorical features, making it convenient for working with datasets that contain various types of variables.\n",
    "\n",
    "#### Flexible Parameter Tuning:\n",
    "\n",
    "LightGBM allows for tuning various hyperparameters, such as the number of trees, learning rate, and tree depth, making it flexible for different scenarios and tasks.\n",
    "\n",
    "In summary, LightGBM is a powerful machine learning algorithm successfully applied to various regression and classification tasks, particularly when efficiency and the ability to handle large datasets are crucial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ab549f-b607-42c9-b347-21659e42f01c",
   "metadata": {},
   "source": [
    "## Data Splitting:\n",
    "\n",
    "The data were divided into three sets: the training set, the validation set, and the test set. \n",
    "\n",
    "#### Training Set:\n",
    "\n",
    "X_train and y_train contain data used for training the model. The model learns the relationships between input features (X_train) and the target variable (y_train).\n",
    "\n",
    "#### Temporary Set:\n",
    "\n",
    "X_temp and y_temp serve as a temporary dataset that will be further split into the validation and test sets.\n",
    "\n",
    "#### Validation and Test Sets:\n",
    "\n",
    "X_val, y_val represent the validation set used to assess the model's performance during training and hyperparameter tuning.\n",
    "X_test, y_test constitute the test set, which remains \"frozen\" until the end of the process and is used for the final evaluation of the model's performance.\n",
    "\n",
    "#### Data Splitting:\n",
    "\n",
    "train_test_split is used twice: first to split the entire dataset (X and y) into the training and temporary sets (with a 60%-40% ratio), and then to split the temporary set into the validation and test sets (with a 50%-50% ratio).\n",
    "\n",
    "The overall approach to dividing the data into training, validation, and test sets allows for an effective evaluation and fine-tuning of the model, minimizing the risk of overfitting, and providing an unbiased assessment of its performance on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c4447b-7bb5-4949-96b9-b2bf99938c2e",
   "metadata": {},
   "source": [
    "## Model Building:\n",
    "\n",
    "Features and the target variable are selected from the dataset (X and y). The features include both categorical and numerical variables related to vehicles.\n",
    "Categorical features are prepared for the model using one-hot encoding, creating a preprocessing pipeline (categorical_pipeline).\n",
    "A ColumnTransformer (preprocessor) is set up to handle both categorical and numerical features separately.\n",
    "\n",
    "#### Model Parameters:\n",
    "\n",
    "* Objective: 'regression' - Indicates that the model is designed for regression tasks.\n",
    "* n_estimators: 21485 - The number of boosting rounds or trees to be built.\n",
    "* learning_rate: 0.037 - The step size shrinkage to prevent overfitting.\n",
    "* num_leaves: 180 - Maximum number of leaves in one tree.\n",
    "* max_depth: -1 - No limit on the depth of the tree.\n",
    "* n_jobs: 7 - The number of parallel threads used for training.\n",
    "* random_state: 42 - Seed for reproducibility.\n",
    "* min_child_samples: 6 - Minimum number of data points in a leaf."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e771748-7b85-40ec-8df3-b2df642d8ad9",
   "metadata": {},
   "source": [
    "## Model Training:\n",
    "The model is trained using the fit method, where the preprocessed training data (X_train_preprocessed and y_train) are used. Early stopping is implemented with a callback that monitors the validation set (X_val_preprocessed and y_val). The training process will stop if there is no improvement in the evaluation metric (default is mean squared error) on the validation set for 150 consecutive rounds.\n",
    "\n",
    "This approach helps prevent overfitting and ensures the model is halted when its performance on the validation set ceases to improve. The training progress is displayed with verbosity during the early stopping process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ca7634-8e08-4914-b32a-9d8a9b9391ea",
   "metadata": {},
   "source": [
    "## Model Evaluation:\n",
    "\n",
    "Training progress information is provided, including the number of rounds and the best iteration based on validation set performance.\n",
    "Model evaluation metrics such as Root Mean Squared Error (RMSE) and R-squared (R^2) are calculated using the test set to assess the model's predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0906f46-d307-4a7a-a2a9-0a3a46d11b9c",
   "metadata": {},
   "source": [
    "## Selection of the Best Model:\n",
    "\n",
    "The calculated performance metrics (RMSE and R^2) provide insights into how well the model predicts prices.\n",
    "The model's predictions are compared to the actual prices, and a DataFrame (comparison_df) is created to analyze differences and percentages of differences.\n",
    "The DataFrame is sorted by the difference percentage, and a random sample (random_comparison_sample) is generated to inspect predictions with the largest discrepancies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b631e9c-5bc3-4645-972d-f212144218d5",
   "metadata": {},
   "source": [
    "## Results:\n",
    "\n",
    "The RMSE is approximately 2889.58, indicating the average prediction error.\n",
    "The R^2 score is approximately 0.968, suggesting a high level of variance explained by the model.\n",
    "The comparison DataFrame shows a sample of actual prices, predicted prices, differences, and difference percentages, providing insights into the model's accuracy.\n",
    "The provided results offer a comprehensive view of the model's predictive capabilities and areas where it might need improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bf66d7-232f-4170-873a-2a0718bc5070",
   "metadata": {},
   "source": [
    "## Conclusions:\n",
    "\n",
    "The Modeling stage utilizing LightGBM has concluded successfully. The model exhibited high predictive accuracy, making it suitable for further deployment in solving the specified business problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
