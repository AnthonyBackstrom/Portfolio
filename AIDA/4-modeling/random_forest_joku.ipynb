{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9bbb0520-0e21-4a9e-bfe0-725895c5bce4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('cleaned_data_200k.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fd862b-8a64-4f83-87a2-a0f4f1ace249",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Creating and training the machine learning model to predict price of the car\n",
    "\n",
    "The initial phase involves selecting a set of features and a target variable from a DataFrame. The chosen features include various car attributes such as body type, fuel economy, engine type, and exterior color, with the car's price being the target variable.\n",
    "\n",
    "Then we preprosess the data. Categorical features undergo One-Hot Encoding, a process that converts these variables into a binary representation suitable for algorithmic processing. Numerical features are left unchanged.\n",
    "\n",
    "The data is then divided into training and testing subsets.\n",
    "\n",
    "The core of this process is the implementation of a LightGBM regressor. We decided to use this model because it delivered superior results compared to other algorithms we tested (random forest, linear regression, catboost and xgboost). It operates by constructing gradient boosting decision trees and employing a gradient-based one-side sampling and exclusive feature bundling, which enhances prediction accuracy and reduces the risks of overfitting. LightGBM is also known for its efficiency with large datasets and speed, making it a robust alternative to traditional random forest models.\n",
    "\n",
    "Upon training the model with the training dataset, it is employed to predict the prices of cars in the test dataset. The evaluation of these predictions is conducted using key performance metrics: Root Mean Squared Error (RMSE), and the R^2 Score. These metrics are integral with assessing the model's predictive accuracy.\n",
    "\n",
    "RMSE tells us how much the prediction differs from actual price in average\n",
    "R^2 Score tells us how accurate the models predictions are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc3d5f6-ea4a-4449-b3db-45eb6e0f688d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Selecting the desired features for the model and the target variable\n",
    "X = df[['body_type', 'city_fuel_economy', 'engine_type', 'exterior_color', 'fuel_tank_volume', 'fuel_type', 'highway_fuel_economy', 'horsepower', 'isCab', 'make_name', 'maximum_seating', 'mileage', 'model_name', 'seller_rating', 'torque', 'transmission', 'wheel_system', 'year', 'damage_history', 'major_options_count'\n",
    "]]  # Features\n",
    "y = df['price']  # Target variable\n",
    "\n",
    "# One-Hot Encoding categorial features\n",
    "categorical_features = ['body_type', 'engine_type', 'damage_history', 'fuel_type', 'isCab', 'make_name', 'transmission', 'wheel_system']\n",
    "numerical_features = ['city_fuel_economy', 'highway_fuel_economy', 'exterior_color', 'fuel_tank_volume', 'horsepower', 'mileage', 'model_name', 'major_options_count', 'seller_rating', 'torque', 'year' ]\n",
    "\n",
    "# Creating preprocessing pipelines for categorical features\n",
    "categorical_pipeline = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))  # Applies One-Hot Encoding\n",
    "])\n",
    "\n",
    "# No transformation for numerical features in this pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_pipeline, categorical_features),\n",
    "        ('num', 'passthrough', numerical_features)  # No changes to numerical features\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Creating a Random Forest model\n",
    "model = RandomForestRegressor(n_estimators=100, min_samples_leaf=1, min_samples_split=2, random_state=42, n_jobs=6)\n",
    "# Consider setting max_depth if necessary!\n",
    "\n",
    "# Creating the final pipeline including preprocessing and the model\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', model)\n",
    "])\n",
    "\n",
    "# Training the model with the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the 'price' for the test data\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Calculating the performance of the predictions\n",
    "rmse = np.sqrt(mean_squared_error(y_test,y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Printing performance metrics\n",
    "print(f'Root Mean Squared Error: {rmse}')\n",
    "print(f'R^2 Score: {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a6ddce10-a2b3-4b98-876e-e35223fbc732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error: 3737.1972354273917\n",
      "R^2 Score: 0.9540277916212505\n"
     ]
    }
   ],
   "source": [
    "# Calculating the performance of the predictions\n",
    "rmse = np.sqrt(mean_squared_error(y_test,y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Printing performance metrics\n",
    "print(f'Root Mean Squared Error: {rmse}')\n",
    "print(f'R^2 Score: {r2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a717cf6-1072-4dca-b967-556dfdc8445e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Tulostetaan satunnaista 15 arvausta sekä autojen oikeat arvot. Tästä voimme katsoa kuinka tarkasti malli ennusti auton arvon 15 satunnaisen auton kohdalla. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "079480de-74ff-4b14-97e1-94aa7235386d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Actual Price  Predicted Price    Difference  Difference%\n",
      "78          19800.0     34151.994750  14351.994750    72.484822\n",
      "13910       39380.0     36595.431408  -2784.568592     7.071022\n",
      "22315       18995.0     19721.952024    726.952024     3.827070\n",
      "901         14968.0     19141.045654   4173.045654    27.879781\n",
      "29743       33980.0     34625.344868    645.344868     1.899190\n",
      "27958       33497.0     32722.442972   -774.557028     2.312318\n",
      "27728       24433.0     23855.664140   -577.335860     2.362935\n",
      "30762       36340.0     35730.650458   -609.349542     1.676801\n",
      "25303       26585.0     27378.418168    793.418168     2.984458\n",
      "11902       71700.0     65914.065170  -5785.934830     8.069644\n",
      "3062        24962.0     29271.539171   4309.539171    17.264399\n",
      "4312        33495.0     28533.820319  -4961.179681    14.811702\n",
      "1595        25120.0     19466.946752  -5653.053248    22.504193\n",
      "799          7795.0     10082.814996   2287.814996    29.349775\n",
      "2183        12995.0     15578.313643   2583.313643    19.879289\n"
     ]
    }
   ],
   "source": [
    "# Comparing predicted prices with actual prices\n",
    "comparison_df = pd.DataFrame({'Actual Price': y_test, 'Predicted Price': y_pred})\n",
    "comparison_df['Difference'] = comparison_df['Predicted Price'] - comparison_df['Actual Price']\n",
    "comparison_df['Difference%'] = np.abs(comparison_df['Difference'] / comparison_df['Actual Price'] * 100)\n",
    "\n",
    "# Sorting the DataFrame by the difference percentage to see the predictions with the biggest discrepancies\n",
    "comparison_df.sort_values(by='Difference%', ascending=False, inplace=True)\n",
    "\n",
    "# Reset index for better readability\n",
    "comparison_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Generate a random sample from the comparison dataframe\n",
    "random_comparison_sample = comparison_df.sample(n=15, random_state=None)  # 'n' is the number of samples\n",
    "\n",
    "# Display the random sample\n",
    "print(random_comparison_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b547f4a8-c254-43c3-9ff9-b503b08d8ebd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
